{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explolation06\n",
        "### 가사.txt를 rnn학습시켜 멋진 가사를 만들어보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 필요한 모듈 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPHcpVTLZAEE",
        "outputId": "fded2e0b-9e77-45da-9f09-d77801a66faa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.2.2\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import tensorflow as tf\n",
        "import os, re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "print(matplotlib.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 가사데이터들을 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tZ6s-HtwyCvi"
      },
      "outputs": [],
      "source": [
        "txt_list = glob.glob('./lyrics/*')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 가져온 데이터들을 리스트형식으로 저장해주기."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2Hj8f-SZtlx",
        "outputId": "5f66d6c6-993d-43c7-bdff-01a025a43e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['I saw you walking by his side heard you whisper all those lies', \"And I couldn't keep from crying\", 'You sang him love songs tenderly that should have been for you and me', \"And I couldn't keep from crying\", 'I saw his eyes drinking your charms while he held you in his arms', 'Him with all his wedding ways rules your heart now in my place', \"I stood and watched him steal a kiss from two lips I know I'll miss\", \"And I couldn't keep from crying I saw his eyes drinking your charms... I love that hair, long an' black\", \"Hangin' down to the middle of your back\"]\n"
          ]
        }
      ],
      "source": [
        "raw_corpus = []\n",
        "\n",
        "\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKTd8Hb1Z4fG",
        "outputId": "21635e72-9a38-407f-a25f-d9bd8e3880bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I saw you walking by his side heard you whisper all those lies\n",
            "And I couldn't keep from crying\n",
            "You sang him love songs tenderly that should have been for you and me\n",
            "And I couldn't keep from crying\n",
            "I saw his eyes drinking your charms while he held you in his arms\n",
            "Him with all his wedding ways rules your heart now in my place\n",
            "I stood and watched him steal a kiss from two lips I know I'll miss\n",
            "And I couldn't keep from crying I saw his eyes drinking your charms... I love that hair, long an' black\n",
            "Hangin' down to the middle of your back\n",
            "Don't cut it off whatever you do\n",
            "I need it to run my fingers through 'Cause you're my baby, uh huh uh, you're my sugar\n",
            "Don't mean maybe, you're my, baby Got me a dollar that I saved\n",
            "Saved it up for a rainy day\n",
            "Everybody's callin' for bills that's due\n",
            "But they don't catch me, I'll spend it on you 'Cause you're my baby, uh huh uh, you're my sugar\n",
            "Don't mean maybe, you're my, baby Got me a guitar, got a six strings\n",
            "And a picker to make 'em ring\n",
            "Every string's gotta know what to do\n",
            "'Cause I'm gonna use 'em to serenade you 'Cause you're my baby, uh huh uh, you're my sugar\n",
            "Don't mean maybe, you're my baby Well, I had me a gal, she said she's mine\n",
            "But she run around on me all the time\n",
            "Now she's gone an' I'm glad we're through\n",
            "'Cause I'm plum-flipped over you 'Cause you're my baby, uh huh uh, you're my sugar\n",
            "Don't mean maybe, you're my, baby Oh, baby, baby, yeah you're my baby\n",
            "Well, I don't mean maybe\n",
            "You drive me crazy\n",
            "I love you, baby, you're my, babydoll Love is a burnin' thing\n",
            "And it makes a fiery ring\n",
            "Bound by wild desire\n",
            "I fell into a ring of fire I fell into a burnin' ring of fire\n",
            "I went down, down, down\n"
          ]
        }
      ],
      "source": [
        "for idx, sentence in enumerate(raw_corpus):\n",
        "    if len(sentence) == 0: continue   \n",
        "  \n",
        "    if idx >30:break\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 잘 가져와진 모습이다. 이제 필요없는 문자열을 삭제해주자\n",
        "## 다음의 코드는 필요없는 문자열을 삭제해주는 함수이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJllpCbNZ7Nd",
        "outputId": "b0279689-a8e1-4d49-f8f3-9a03de61bf7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start> this is sample sentence . <end>\n"
          ]
        }
      ],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() \n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) \n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) \n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    sentence = '<start> ' + sentence + ' <end>' \n",
        "    return sentence\n",
        "\n",
        "\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuy-Qe6aZ98C",
        "outputId": "786c202d-1b47-4f6c-b43d-85fe00814235"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<start> i saw you walking by his side heard you whisper all those lies <end>',\n",
              " '<start> and i couldn t keep from crying <end>',\n",
              " '<start> you sang him love songs tenderly that should have been for you and me <end>',\n",
              " '<start> and i couldn t keep from crying <end>',\n",
              " '<start> i saw his eyes drinking your charms while he held you in his arms <end>',\n",
              " '<start> him with all his wedding ways rules your heart now in my place <end>',\n",
              " '<start> i stood and watched him steal a kiss from two lips i know i ll miss <end>',\n",
              " '<start> and i couldn t keep from crying i saw his eyes drinking your charms . . . i love that hair , long an black <end>',\n",
              " '<start> hangin down to the middle of your back <end>',\n",
              " '<start> don t cut it off whatever you do <end>']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "   \n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "    \n",
        "    \n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus.append(preprocessed_sentence)\n",
        "        \n",
        "\n",
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 잘 삭제된 모습이다. 이제 이 데이터를 토큰화하는 함수로 토큰화해보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph1hfcDzaAcq",
        "outputId": "a3f52f3e-be61-4064-f054-0f2412b8ee91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[   2    5  475 ...    0    0    0]\n",
            " [   2    8    5 ...    0    0    0]\n",
            " [   2    7 1731 ...    0    0    0]\n",
            " ...\n",
            " [   2    5   22 ...    0    0    0]\n",
            " [   2    5   22 ...    0    0    0]\n",
            " [   2    5   22 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7feb567f18d0>\n"
          ]
        }
      ],
      "source": [
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    \n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 잘 토큰화가 된 모습이다. 필자는 단어장을 12000으로 해보았다. 보통 7000~8000개를 한다고 한다.\n",
        "## 그리고 나는 maxlen값을 쓰지않았다. 학습시간이 클것같긴하지만 나는 문장의 수가 많으면 많을수록 좋다고 가정을 했다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWC2K86IeF1J",
        "outputId": "37174f58-13a6-45d3-f28b-3c052dc74f2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[   2    5  475    7  775  122  105  300  297    7]\n",
            " [   2    8    5  601   15  129   74  878    3    0]\n",
            " [   2    7 1731  142   33  903 4376   17  198   76]]\n"
          ]
        }
      ],
      "source": [
        "print(tensor[:3, :10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bga0HWu2eHt9",
        "outputId": "d3928adc-11c7-453e-9e4b-32b5fd35084f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : ,\n",
            "5 : i\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : a\n",
            "10 : to\n"
          ]
        }
      ],
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 토큰화도 잘 된 모습이다. 이제 발리데이션 로스를 확인하기위해 트레인과 테스트 데이터로 분리해주자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldL7loWGeP6v",
        "outputId": "320fa0c5-6938-420f-a15a-f9b7320387f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[   2    5  475    7  775  122  105  300  297    7 1710   24  423  709\n",
            "    3    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "[   5  475    7  775  122  105  300  297    7 1710   24  423  709    3\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "src_input = tensor[:, :-1]  \n",
        "\n",
        "tgt_input = tensor[:, 1:]   \n",
        "(enc_train, enc_val, dec_train, dec_val) = train_test_split(src_input,tgt_input,test_size=0.2,shuffle=True,random_state=500)\n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 여기서 필자는 배치사이즈를 128로 조정해주었다. 256이상부터는 아웃오브메모리 즉, 메모리가 부족한 상태가 뜨길래 128로 해 주었다. 아까 maxlen의 값을 안해줘서 그런것같다.\n",
        "## 그래도 배치사이즈를 조정해 오류를 없앴다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ashy1Oy3eRXm",
        "outputId": "c7fba769-187d-4b2c-8861-f260dc4f2937"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(128, 346), dtype=tf.int32, name=None), TensorSpec(shape=(128, 346), dtype=tf.int32, name=None))>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 128\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
        "dataset_test = dataset_test.shuffle(BUFFER_SIZE)\n",
        "dataset_test = dataset_test.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 이제 데이터를 학습시키기 위해 모델링을 해주자. 필자는 임베딩1층, rnn2층, 리니어1층으로 모델링을 해주었으며, 임베딩 사이즈는 512, 은닉층의 뉴런갯수는 1024개로 해주었다.\n",
        "## 아무래도 뉴런의 갯수가 많아지면 많아 질수록 학습시간이 오래걸릴것같다. 하지만 그만큼 학습률은 좋을듯하다. 다른 단점으로는 과적합이 잘 일어날것같은데 epoch수를 작게해서 모델을 돌려보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VXI1_tXOeTVs"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 512\n",
        "hidden_size = 1024\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 샘플로 한 배치만 데이터를 모델에 넣어보았다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wxavpp_OeVFw",
        "outputId": "02838f0d-ac60-4fc6-e540-43c93684a9af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(128, 346, 12001), dtype=float32, numpy=\n",
              "array([[[ 1.59427596e-04, -1.13546484e-04, -1.34676869e-04, ...,\n",
              "          1.19838805e-04,  1.27121268e-04, -2.10279672e-04],\n",
              "        [ 3.90956120e-04, -2.11121387e-05, -2.25890399e-04, ...,\n",
              "          1.93930435e-04, -1.35101873e-04, -2.61748180e-04],\n",
              "        [ 5.03247429e-04,  4.13110189e-04, -3.06453148e-04, ...,\n",
              "          7.03537138e-04, -5.65044211e-05, -3.17306491e-04],\n",
              "        ...,\n",
              "        [ 4.58456983e-04, -3.47380713e-03, -4.75838920e-03, ...,\n",
              "          1.03367587e-04, -1.59916433e-03, -2.38659602e-04],\n",
              "        [ 4.58457042e-04, -3.47380619e-03, -4.75838827e-03, ...,\n",
              "          1.03367005e-04, -1.59916386e-03, -2.38659370e-04],\n",
              "        [ 4.58456809e-04, -3.47380480e-03, -4.75838874e-03, ...,\n",
              "          1.03367005e-04, -1.59916386e-03, -2.38659370e-04]],\n",
              "\n",
              "       [[ 1.59427596e-04, -1.13546484e-04, -1.34676869e-04, ...,\n",
              "          1.19838805e-04,  1.27121268e-04, -2.10279672e-04],\n",
              "        [ 4.36619535e-04, -2.92623066e-04, -1.22517973e-04, ...,\n",
              "         -1.58109397e-04,  1.98626483e-04, -5.51094941e-04],\n",
              "        [ 8.70867632e-04, -2.90725031e-04, -5.19732675e-05, ...,\n",
              "         -1.57716058e-04,  4.73931752e-04, -5.77877450e-04],\n",
              "        ...,\n",
              "        [ 4.58456954e-04, -3.47380573e-03, -4.75838967e-03, ...,\n",
              "          1.03366161e-04, -1.59916456e-03, -2.38657842e-04],\n",
              "        [ 4.58457187e-04, -3.47380526e-03, -4.75839060e-03, ...,\n",
              "          1.03365812e-04, -1.59916456e-03, -2.38658540e-04],\n",
              "        [ 4.58457536e-04, -3.47380573e-03, -4.75839106e-03, ...,\n",
              "          1.03366066e-04, -1.59916503e-03, -2.38658773e-04]],\n",
              "\n",
              "       [[ 1.59427596e-04, -1.13546484e-04, -1.34676869e-04, ...,\n",
              "          1.19838805e-04,  1.27121268e-04, -2.10279672e-04],\n",
              "        [ 2.85854854e-04, -2.61196052e-04, -2.34666368e-04, ...,\n",
              "          4.68548009e-04,  4.36336559e-04, -6.06783608e-04],\n",
              "        [ 6.36328070e-04, -6.70455222e-04, -4.57107439e-04, ...,\n",
              "          1.06855389e-03,  7.60223891e-04, -5.53615333e-04],\n",
              "        ...,\n",
              "        [ 4.58455965e-04, -3.47380713e-03, -4.75839060e-03, ...,\n",
              "          1.03366539e-04, -1.59916433e-03, -2.38657958e-04],\n",
              "        [ 4.58456489e-04, -3.47380666e-03, -4.75839153e-03, ...,\n",
              "          1.03366947e-04, -1.59916538e-03, -2.38657027e-04],\n",
              "        [ 4.58457012e-04, -3.47380666e-03, -4.75839153e-03, ...,\n",
              "          1.03366576e-04, -1.59916573e-03, -2.38656794e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 1.59427596e-04, -1.13546484e-04, -1.34676869e-04, ...,\n",
              "          1.19838805e-04,  1.27121268e-04, -2.10279672e-04],\n",
              "        [ 2.85641698e-04, -5.74734120e-04,  5.24631141e-05, ...,\n",
              "         -1.55853340e-04,  5.79663785e-04, -1.51440254e-05],\n",
              "        [ 2.20625589e-04, -6.28552400e-04,  3.52429808e-04, ...,\n",
              "         -5.51053090e-04,  7.42921373e-04,  3.83247010e-04],\n",
              "        ...,\n",
              "        [ 4.58457129e-04, -3.47380666e-03, -4.75838687e-03, ...,\n",
              "          1.03365652e-04, -1.59916608e-03, -2.38657332e-04],\n",
              "        [ 4.58456896e-04, -3.47380573e-03, -4.75838734e-03, ...,\n",
              "          1.03365368e-04, -1.59916654e-03, -2.38657100e-04],\n",
              "        [ 4.58457362e-04, -3.47380713e-03, -4.75838734e-03, ...,\n",
              "          1.03365892e-04, -1.59916736e-03, -2.38656648e-04]],\n",
              "\n",
              "       [[ 1.59427596e-04, -1.13546484e-04, -1.34676869e-04, ...,\n",
              "          1.19838805e-04,  1.27121268e-04, -2.10279672e-04],\n",
              "        [ 1.27799605e-04, -3.00833053e-04,  1.76253612e-04, ...,\n",
              "          1.51306085e-04,  1.12639878e-04, -2.29212834e-04],\n",
              "        [ 1.53957546e-04, -4.67291044e-04,  2.62791669e-04, ...,\n",
              "          4.36252216e-04,  3.68371373e-04, -5.98407583e-04],\n",
              "        ...,\n",
              "        [ 4.58457303e-04, -3.47380619e-03, -4.75839106e-03, ...,\n",
              "          1.03366161e-04, -1.59916445e-03, -2.38658278e-04],\n",
              "        [ 4.58457536e-04, -3.47380666e-03, -4.75839153e-03, ...,\n",
              "          1.03366299e-04, -1.59916421e-03, -2.38658278e-04],\n",
              "        [ 4.58458002e-04, -3.47380666e-03, -4.75839153e-03, ...,\n",
              "          1.03366190e-04, -1.59916410e-03, -2.38657536e-04]],\n",
              "\n",
              "       [[ 1.59427596e-04, -1.13546484e-04, -1.34676869e-04, ...,\n",
              "          1.19838805e-04,  1.27121268e-04, -2.10279672e-04],\n",
              "        [ 1.18686788e-04, -6.10007439e-04, -1.88316495e-04, ...,\n",
              "          9.79304605e-05,  1.63498116e-04, -4.91176965e-04],\n",
              "        [-3.90625137e-05, -7.63991207e-04, -1.30178611e-04, ...,\n",
              "          3.83984880e-04,  3.42004292e-04, -9.07079782e-04],\n",
              "        ...,\n",
              "        [ 4.58457071e-04, -3.47380619e-03, -4.75838920e-03, ...,\n",
              "          1.03365041e-04, -1.59916701e-03, -2.38657260e-04],\n",
              "        [ 4.58457187e-04, -3.47380573e-03, -4.75838920e-03, ...,\n",
              "          1.03365332e-04, -1.59916724e-03, -2.38657027e-04],\n",
              "        [ 4.58456954e-04, -3.47380573e-03, -4.75838920e-03, ...,\n",
              "          1.03365215e-04, -1.59916701e-03, -2.38657027e-04]]],\n",
              "      dtype=float32)>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "\n",
        "model(src_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 층을 잘 쌓았나 확인해본뒤 본격적으로 학습시켜보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BNUrpj_eWzO",
        "outputId": "7fc9ebc0-5dd4-4353-e8f1-abd0fd031de7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"text_generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  6144512   \n",
            "                                                                 \n",
            " lstm (LSTM)                 multiple                  6295552   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               multiple                  8392704   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  12301025  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33,133,793\n",
            "Trainable params: 33,133,793\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP6Ao26ZTJ-_",
        "outputId": "f76a0213-2acb-48d9-b2c1-9bd2389caeb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1098/1098 [==============================] - 2558s 2s/step - loss: 0.1892 - val_loss: 0.1417\n",
            "Epoch 2/5\n",
            "1098/1098 [==============================] - 2577s 2s/step - loss: 0.1347 - val_loss: 0.1307\n",
            "Epoch 3/5\n",
            "1098/1098 [==============================] - 2570s 2s/step - loss: 0.1244 - val_loss: 0.1234\n",
            "Epoch 4/5\n",
            "1098/1098 [==============================] - 2571s 2s/step - loss: 0.1152 - val_loss: 0.1173\n",
            "Epoch 5/5\n",
            "1098/1098 [==============================] - 2591s 2s/step - loss: 0.1056 - val_loss: 0.1124\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7feb5563f690>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(dataset, epochs=5, validation_data =dataset_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 학습결과 과적합은 일어나보이지않은 모습이며 loss값 발리데이션 loss값이 다 낮은 모습이다. 이제 가사를 뽑아보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_eBhiCM9-UzB"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "  \n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "  \n",
        "    while True:\n",
        "        \n",
        "        predict = model(test_tensor) \n",
        "        \n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        \n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "   \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Vi0pdkKDNqLL",
        "outputId": "ae6e9a5a-e890-495c-9366-e93d8ac607e4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> he s a <unk> , he s a mess , he s a monster <end> '"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ㅋㅋㅋ뭔가 랩가사를 만들어준것같다. 그는 엉망이고, 그는 괴물이고, 어느 여인이 남자를 원망하는 가사인것같다. 연애할때 남자가 잘 못해주었나보다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 이번에는 가사데이터를 통해 가사를 만들어봤는데, rnn의 모델중 lstm은 엄청 복잡한 모델인것같다. 그래서 학습시간도 긴것같다. 하지만 그만큼 성능은 좋은듯 보인다. 이것으로 Explolation06  과제를 마친다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Untitled8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
